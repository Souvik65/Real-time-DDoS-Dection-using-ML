{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6211501,"sourceType":"datasetVersion","datasetId":3566887},{"sourceId":6878499,"sourceType":"datasetVersion","datasetId":3952124}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Version 1","metadata":{}},{"cell_type":"code","source":"# !pip uninstall -y scikit-learn\n# !pip install scikit-learn==1.8.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# os.remove(\"/kaggle/working/ddos_model_realtime.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport pandas as pd\nimport joblib\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\n# --------------------------------------------------\n# CONFIG\n# --------------------------------------------------\nDATA_PATH = '/kaggle/input/ddos-evaluation-dataset-cic-ddos2019'\nSAMPLE_FRACTION = 0.1        # Increase if RAM allows\nCHUNKSIZE = 10_000\nN_ESTIMATORS = 150\nRANDOM_STATE = 42\nMODEL_PATH = '/kaggle/working/ddos_model.pkl'\n\n# --------------------------------------------------\n# STEP 1: LOAD DATASET (CHUNKED + SAMPLED)\n# --------------------------------------------------\ncsv_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.csv')]\nif not csv_files:\n    raise RuntimeError(\"No CSV files found in dataset path\")\n\ndataframes = []\n\nfor file in csv_files:\n    print(f\"Loading {file}\")\n    file_path = os.path.join(DATA_PATH, file)\n\n    for chunk in pd.read_csv(file_path, chunksize=CHUNKSIZE, low_memory=False):\n        chunk.columns = chunk.columns.str.strip()\n        sampled = chunk.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_STATE)\n        dataframes.append(sampled)\n\ndf = pd.concat(dataframes, ignore_index=True)\nprint(f\"Dataset loaded: {df.shape}\")\n\n# --------------------------------------------------\n# STEP 2: PREPROCESS\n# --------------------------------------------------\nif 'Label' not in df.columns:\n    raise KeyError(f\"'Label' column not found. Available columns:\\n{df.columns.tolist()}\")\n\ndf['Label'] = df['Label'].astype(str).str.lower().apply(\n    lambda x: 0 if x == 'benign' else 1\n)\n\nFEATURES = [\n    'Source IP',\n    'Destination IP',\n    'Source Port',\n    'Destination Port',\n    'Protocol',\n    'Flow Duration',\n    'Total Fwd Packets',\n    'Total Backward Packets',\n    'Fwd Packet Length Max',\n    'Bwd Packet Length Max',\n    'SYN Flag Count',\n    'ACK Flag Count'\n]\n\nAVAILABLE_FEATURES = [f for f in FEATURES if f in df.columns]\nif not AVAILABLE_FEATURES:\n    raise RuntimeError(\"None of the requested features exist in the dataset\")\n\nX = df[AVAILABLE_FEATURES].copy()\ny = df['Label']\n\n# Encode IPs safely\nfor col in ['Source IP', 'Destination IP']:\n    if col in X.columns:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col].astype(str))\n\nX = X.fillna(0)\n\nprint(f\"Preprocessing complete. X shape: {X.shape}\")\n\n# --------------------------------------------------\n# STEP 3: TRAIN / TEST SPLIT\n# --------------------------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    stratify=y,\n    random_state=RANDOM_STATE\n)\n\n# --------------------------------------------------\n# STEP 4: TRAIN MODEL\n# --------------------------------------------------\nmodel = RandomForestClassifier(\n    n_estimators=N_ESTIMATORS,\n    n_jobs=-1,\n    class_weight='balanced',\n    random_state=RANDOM_STATE\n)\n\nprint(\"Training model...\")\nmodel.fit(X_train, y_train)\n\n# --------------------------------------------------\n# STEP 5: EVALUATE\n# --------------------------------------------------\ny_pred = model.predict(X_test)\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_test, y_pred, digits=4))\n\n# --------------------------------------------------\n# STEP 6: SAVE MODEL\n# --------------------------------------------------\njoblib.dump(model, MODEL_PATH)\nprint(f\"Model saved to {MODEL_PATH}\")\n\n\n ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sklearn\nprint(\"sklearn version:\", sklearn.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# version 2","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# -----------------------------------\n# DATASET ROOT\n# -----------------------------------\n\nDATA_ROOT = \"/kaggle/input/datasets/rodrigorosasilva/cic-ddos2019-30gb-full-dataset-csv-files\"\n\n\ncsv_files = []\nfor root, dirs, files in os.walk(DATA_ROOT):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_files.append(os.path.join(root, file))\n\nprint(f\"Found {len(csv_files)} CSV files\")\n\n# -----------------------------------\n# LOAD DATA (Progress Bar)\n# -----------------------------------\n\ndf_list = []\nprint(\"\\nðŸ”„ Loading CSV files...\")\n\nfor file in tqdm(csv_files, desc=\"Loading Files\"):\n    temp_df = pd.read_csv(file, low_memory=False, nrows=10000)\n    df_list.append(temp_df)\n\ndf = pd.concat(df_list, ignore_index=True)\nprint(\"Dataset shape:\", df.shape)\n\n# -----------------------------------\n# CLEAN DATA\n# -----------------------------------\n\nprint(\"\\nðŸ§¹ Cleaning dataset...\")\n\ndf.columns = df.columns.str.strip()\n\ndrop_cols = [\"Flow ID\", \"Source IP\", \"Destination IP\", \"Timestamp\", \"Unnamed: 0\"]\nfor col in drop_cols:\n    if col in df.columns:\n        df.drop(columns=[col], inplace=True)\n\nif \"Label\" not in df.columns:\n    raise ValueError(\"Label column missing\")\n\ndf[\"Label\"] = df[\"Label\"].astype(\"category\").cat.codes\ndf = df.select_dtypes(include=[np.number])\n\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\n\nX = df.drop(columns=[\"Label\"])\ny = df[\"Label\"]\n\n# -----------------------------------\n# SPLIT\n# -----------------------------------\n\nprint(\"\\nâœ‚ Splitting dataset...\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# -----------------------------------\n# SMOTE\n# -----------------------------------\n\nprint(\"\\nâš– Applying SMOTE...\")\nsmote = SMOTE(random_state=42)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n# -----------------------------------\n# DEFINE MODELS\n# -----------------------------------\n\nextra = ExtraTreesClassifier(\n    n_estimators=100,\n    max_depth=15,\n    random_state=42,\n    n_jobs=-1\n)\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    eval_metric=\"mlogloss\",\n    random_state=42,\n    verbosity=0\n)\n\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=100,\n    max_depth=6,\n    num_leaves=31,\n    random_state=42,\n    verbosity=-1\n)\n\n\n\n# -----------------------------------\n# PIPELINE\n# -----------------------------------\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=30)),\n    (\"classifier\", VotingClassifier(\n        estimators=[\n            (\"extra\", extra),\n            (\"xgb\", xgb_model),\n            (\"lgb\", lgb_model)\n        ],\n        voting=\"soft\",\n        n_jobs=-1\n    ))\n])\n\n# -----------------------------------\n# TRAIN WITH PROGRESS BAR\n# -----------------------------------\n\nprint(\"\\nðŸš€ Training pipeline...\\n\")\n\ntotal_steps = 3\n\nwith tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n\n    # Step 1: Scaling\n    pbar.set_description(\"Scaling Data\")\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_train_bal)\n    pbar.update(1)\n\n    # Step 2: PCA\n    pbar.set_description(\"Applying PCA\")\n    pca = PCA(n_components=30)\n    X_pca = pca.fit_transform(X_scaled)\n    pbar.update(1)\n\n    # Step 3: Train Ensemble\n    pbar.set_description(\"Training Ensemble (ExtraTrees + XGB + LGB)\")\n    pipeline.named_steps[\"classifier\"].fit(X_pca, y_train_bal)\n    pbar.update(1)\n\nprint(\"\\nâœ… Training complete\")\n\n# -----------------------------------\n# EVALUATE\n# -----------------------------------\n\nprint(\"\\nðŸ“Š Evaluating model...\")\n\nX_test_scaled = scaler.transform(X_test)\nX_test_pca = pca.transform(X_test_scaled)\n\ny_pred = pipeline.named_steps[\"classifier\"].predict(X_test_pca)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# -----------------------------------\n# SAVE MODEL (FULL PIPELINE)\n# -----------------------------------\n\nfinal_pipeline = Pipeline([\n    (\"scaler\", scaler),\n    (\"pca\", pca),\n    (\"classifier\", pipeline.named_steps[\"classifier\"])\n])\n\njoblib.dump(final_pipeline, \"/kaggle/working/ddos_model-latest-1.pkl\", compress=3)\n\nprint(\"\\nðŸ”¥ Model saved as ddos_model_latest.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip uninstall -y imbalanced-learn\n# !pip install imbalanced-learn --upgrade\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# version 3","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# -----------------------------------\n# DATASET ROOT\n# -----------------------------------\n\nDATA_ROOT = \"/kaggle/input/datasets/rodrigorosasilva/cic-ddos2019-30gb-full-dataset-csv-files\"\n\ncsv_files = []\nfor root, dirs, files in os.walk(DATA_ROOT):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_files.append(os.path.join(root, file))\n\nprint(f\"Found {len(csv_files)} CSV files\")\n\n# -----------------------------------\n# LOAD DATA\n# -----------------------------------\n\ndf_list = []\nprint(\"\\nðŸ”„ Loading CSV files...\")\n\nfor file in tqdm(csv_files, desc=\"Loading Files\"):\n    temp_df = pd.read_csv(file, low_memory=False, nrows=10000)\n    df_list.append(temp_df)\n\ndf = pd.concat(df_list, ignore_index=True)\nprint(\"Dataset shape:\", df.shape)\n\n# -----------------------------------\n# CLEAN DATA\n# -----------------------------------\n\nprint(\"\\nðŸ§¹ Cleaning dataset...\")\n\ndf.columns = df.columns.str.strip()\n\ndrop_cols = [\"Flow ID\", \"Source IP\", \"Destination IP\", \"Timestamp\", \"Unnamed: 0\"]\nfor col in drop_cols:\n    if col in df.columns:\n        df.drop(columns=[col], inplace=True)\n\nif \"Label\" not in df.columns:\n    raise ValueError(\"Label column missing\")\n\ndf[\"Label\"] = df[\"Label\"].astype(\"category\").cat.codes\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\n\n# -----------------------------------\n# SELECT ONLY DETECTOR FEATURES\n# -----------------------------------\n\ndetector_features = [\n    \"Flow Duration\",\n    \"Total Fwd Packets\",\n    \"Total Backward Packets\",\n    \"Total Length of Fwd Packets\",\n    \"Total Length of Bwd Packets\",\n    \"Flow Bytes/s\",\n    \"Flow Packets/s\",\n    \"Fwd Packet Length Mean\",\n    \"Fwd Packet Length Std\",\n    \"Bwd Packet Length Mean\",\n    \"Bwd Packet Length Std\",\n    \"Fwd IAT Mean\",\n    \"Fwd IAT Std\",\n    \"Bwd IAT Mean\",\n    \"Bwd IAT Std\",\n    \"SYN Flag Count\",\n    \"ACK Flag Count\",\n    \"FIN Flag Count\",\n    \"RST Flag Count\",\n    \"PSH Flag Count\",\n    \"URG Flag Count\"\n]\n\nmissing = [col for col in detector_features if col not in df.columns]\nif missing:\n    raise Exception(f\"Missing required columns: {missing}\")\n\nX = df[detector_features]\ny = df[\"Label\"]\n\n# -----------------------------------\n# SPLIT\n# -----------------------------------\n\nprint(\"\\nâœ‚ Splitting dataset...\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# -----------------------------------\n# SMOTE\n# -----------------------------------\n\nprint(\"\\nâš– Applying SMOTE...\")\nsmote = SMOTE(random_state=42)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n# -----------------------------------\n# DEFINE MODELS\n# -----------------------------------\n\nextra = ExtraTreesClassifier(\n    n_estimators=150,\n    max_depth=20,\n    random_state=42,\n    n_jobs=-1\n)\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=150,\n    max_depth=6,\n    learning_rate=0.1,\n    eval_metric=\"mlogloss\",\n    random_state=42,\n    verbosity=0\n)\n\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=150,\n    max_depth=6,\n    num_leaves=31,\n    random_state=42,\n    verbosity=-1\n)\n\n# -----------------------------------\n# PIPELINE (NO PCA NOW)\n# -----------------------------------\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"classifier\", VotingClassifier(\n        estimators=[\n            (\"extra\", extra),\n            (\"xgb\", xgb_model),\n            (\"lgb\", lgb_model)\n        ],\n        voting=\"soft\",\n        n_jobs=-1\n    ))\n])\n\n# -----------------------------------\n# TRAIN\n# -----------------------------------\n\nprint(\"\\nðŸš€ Training model...\\n\")\npipeline.fit(X_train_bal, y_train_bal)\n\nprint(\"\\nâœ… Training complete\")\n\n# -----------------------------------\n# EVALUATE\n# -----------------------------------\n\nprint(\"\\nðŸ“Š Evaluating model...\")\n\ny_pred = pipeline.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# -----------------------------------\n# SAVE\n# -----------------------------------\n\njoblib.dump(pipeline, \"/kaggle/working/ddos_model_realtime.pkl\", compress=3)\n\nprint(\"\\nðŸ”¥ Model saved as ddos_model_realtime.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T15:12:09.020183Z","iopub.execute_input":"2026-02-11T15:12:09.020448Z","iopub.status.idle":"2026-02-11T15:13:32.182426Z","shell.execute_reply.started":"2026-02-11T15:12:09.020424Z","shell.execute_reply":"2026-02-11T15:13:32.181756Z"}},"outputs":[{"name":"stdout","text":"Found 18 CSV files\n\nðŸ”„ Loading CSV files...\n","output_type":"stream"},{"name":"stderr","text":"Loading Files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:01<00:00, 10.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Dataset shape: (180000, 88)\n\nðŸ§¹ Cleaning dataset...\n\nâœ‚ Splitting dataset...\n\nâš– Applying SMOTE...\n\nðŸš€ Training model...\n\n\nâœ… Training complete\n\nðŸ“Š Evaluating model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2691: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.5955487630146146\n              precision    recall  f1-score   support\n\n           0       0.99      0.97      0.98      4135\n           1       0.98      0.80      0.88      2733\n           2       0.33      0.64      0.43      2946\n           3       0.39      0.08      0.14      2958\n           4       0.70      0.95      0.80       601\n           5       0.57      0.61      0.59      2902\n           6       0.52      0.23      0.32      2993\n           7       0.48      0.93      0.63      2979\n           8       0.37      0.69      0.48      2932\n           9       0.34      0.01      0.02      2948\n          10       0.59      0.52      0.55      2854\n          11       0.61      0.90      0.73      5717\n          12       0.64      0.22      0.33      1525\n          13       0.98      0.71      0.82      5563\n          14       0.62      1.00      0.76      2624\n          15       0.44      0.28      0.34      2942\n          16       0.42      0.21      0.28      2993\n\n    accuracy                           0.60     52345\n   macro avg       0.59      0.57      0.53     52345\nweighted avg       0.61      0.60      0.56     52345\n\n\nðŸ”¥ Model saved as ddos_model_realtime.pkl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# final version","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# -----------------------------------\n# DATASET ROOT\n# -----------------------------------\n\nDATA_ROOT = \"/kaggle/input/datasets/rodrigorosasilva/cic-ddos2019-30gb-full-dataset-csv-files\"\n# DATA_ROOT = \"/kaggle/input/ddos-evaluation-dataset-cic-ddos2019\"\n\ncsv_files = []\nfor root, dirs, files in os.walk(DATA_ROOT):\n    for file in files:\n        if file.endswith(\".csv\"):\n            csv_files.append(os.path.join(root, file))\n\nprint(f\"Found {len(csv_files)} CSV files\")\n\n# -----------------------------------\n# LOAD DATA (Progress Bar)\n# -----------------------------------\n\ndf_list = []\nprint(\"\\nðŸ”„ Loading CSV files...\")\n\nfor file in tqdm(csv_files, desc=\"Loading Files\"):\n    temp_df = pd.read_csv(file, low_memory=False, nrows=200000)\n    df_list.append(temp_df)\n\ndf = pd.concat(df_list, ignore_index=True)\nprint(\"Dataset shape:\", df.shape)\n\n# -----------------------------------\n# FIXED: SELECT ONLY FEATURES USED IN DETECTION\n# -----------------------------------\n\n# Define the exact features computed in detection (to avoid mismatch)\ndetection_features = [\n    \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\",\n    \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\",\n    \"Flow Bytes/s\", \"Flow Packets/s\", \"Fwd Packet Length Mean\",\n    \"Fwd Packet Length Std\", \"Bwd Packet Length Mean\", \"Bwd Packet Length Std\",\n    \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Bwd IAT Mean\", \"Bwd IAT Std\",\n    \"SYN Flag Count\", \"ACK Flag Count\", \"FIN Flag Count\",\n    \"RST Flag Count\", \"PSH Flag Count\", \"URG Flag Count\"\n]\n\n# Clean and process Label first\ndf.columns = df.columns.str.strip()\ndrop_cols = [\"Flow ID\", \"Source IP\", \"Destination IP\", \"Timestamp\", \"Unnamed: 0\"]\nfor col in drop_cols:\n    if col in df.columns:\n        df.drop(columns=[col], inplace=True)\n\nif \"Label\" not in df.columns:\n    raise ValueError(\"Label column missing\")\n\ndf[\"Label\"] = df[\"Label\"].astype(\"category\").cat.codes  # Convert to numeric codes\n\n# Now select numeric columns (includes Label now)\ndf = df.select_dtypes(include=[np.number])\n\n# Select only detection features + Label (exclude Label from available_features)\navailable_features = [col for col in detection_features if col in df.columns]\ndf = df[available_features + [\"Label\"]]  # Keep only matching features + label\n\nprint(f\"Selected {len(available_features)} features for training: {available_features}\")\n\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\n\nX = df.drop(columns=[\"Label\"])\ny = df[\"Label\"]\n\n# -----------------------------------\n# SPLIT\n# -----------------------------------\n\nprint(\"\\nâœ‚ Splitting dataset...\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# -----------------------------------\n# SMOTE\n# -----------------------------------\n\nprint(\"\\nâš– Applying SMOTE...\")\nsmote = SMOTE(random_state=42)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n# -----------------------------------\n# DEFINE MODELS\n# -----------------------------------\n\nextra = ExtraTreesClassifier(\n    n_estimators=100,\n    max_depth=15,\n    random_state=42,\n    n_jobs=-1\n)\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    eval_metric=\"mlogloss\",\n    random_state=42,\n    verbosity=0\n)\n\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=100,\n    max_depth=6,\n    num_leaves=31,\n    random_state=42,\n    verbosity=-1\n)\n\n# -----------------------------------\n# PIPELINE (Fixed: Fit the full pipeline, not pieces)\n# -----------------------------------\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA(n_components=min(30, len(available_features)))),  # Adjust PCA if fewer features\n    (\"classifier\", VotingClassifier(\n        estimators=[\n            (\"extra\", extra),\n            (\"xgb\", xgb_model),\n            (\"lgb\", lgb_model)\n        ],\n        voting=\"soft\",\n        n_jobs=-1\n    ))\n])\n\n# -----------------------------------\n# TRAIN (Fixed: Fit the entire pipeline at once)\n# -----------------------------------\n\nprint(\"\\nðŸš€ Training pipeline...\\n\")\n\nstart_time = time.time()\npipeline.fit(X_train_bal, y_train_bal)  # Fit the whole pipeline\ntraining_time = time.time() - start_time\n\nprint(f\"âœ… Training complete in {training_time:.2f} seconds\")\n\n# -----------------------------------\n# EVALUATE\n# -----------------------------------\n\nprint(\"\\nðŸ“Š Evaluating model...\")\n\ny_pred = pipeline.predict(X_test)\n\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# -----------------------------------\n# SAVE MODEL (FULL PIPELINE)\n# -----------------------------------\n\njoblib.dump(pipeline, \"/kaggle/working/ddos_model-latest-1.pkl\", compress=3)\n\nprint(\"\\nðŸ”¥ Model saved as ddos_model-latest-1.pkl\")\nprint(f\"Feature names in model: {pipeline.feature_names_in_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T18:21:14.379029Z","iopub.execute_input":"2026-02-13T18:21:14.379303Z","iopub.status.idle":"2026-02-13T18:34:07.649192Z","shell.execute_reply.started":"2026-02-13T18:21:14.379277Z","shell.execute_reply":"2026-02-13T18:34:07.648361Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"name":"stdout","text":"Found 18 CSV files\n\nðŸ”„ Loading CSV files...\n","output_type":"stream"},{"name":"stderr","text":"Loading Files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:52<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Dataset shape: (3591694, 88)\nSelected 21 features for training: ['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Flow Bytes/s', 'Flow Packets/s', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Fwd IAT Mean', 'Fwd IAT Std', 'Bwd IAT Mean', 'Bwd IAT Std', 'SYN Flag Count', 'ACK Flag Count', 'FIN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'URG Flag Count']\n\nâœ‚ Splitting dataset...\n\nâš– Applying SMOTE...\n\nðŸš€ Training pipeline...\n\nâœ… Training complete in 596.89 seconds\n\nðŸ“Š Evaluating model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.7393902938466264\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98      7904\n           1       0.75      0.73      0.74      4849\n           2       0.29      0.68      0.40       371\n           3       0.31      0.11      0.16      1299\n           4       1.00      0.96      0.98     52260\n           5       0.16      0.36      0.22       824\n           6       0.12      0.52      0.20       141\n           7       0.32      0.25      0.28     18122\n           8       0.32      0.49      0.38     21956\n           9       0.05      0.55      0.09        65\n          10       0.42      0.57      0.48      1495\n          11       0.30      0.51      0.38       266\n          12       0.09      0.34      0.14       322\n          13       0.99      0.92      0.96     52682\n          14       0.85      0.99      0.91     15482\n          15       0.50      0.43      0.47     32362\n          16       0.90      0.69      0.78     23818\n          17       0.09      0.74      0.15        93\n          18       0.12      0.91      0.21        97\n\n    accuracy                           0.74    234408\n   macro avg       0.45      0.62      0.47    234408\nweighted avg       0.77      0.74      0.75    234408\n\n\nðŸ”¥ Model saved as ddos_model-latest-1.pkl\nFeature names in model: ['Flow Duration' 'Total Fwd Packets' 'Total Backward Packets'\n 'Total Length of Fwd Packets' 'Total Length of Bwd Packets'\n 'Flow Bytes/s' 'Flow Packets/s' 'Fwd Packet Length Mean'\n 'Fwd Packet Length Std' 'Bwd Packet Length Mean' 'Bwd Packet Length Std'\n 'Fwd IAT Mean' 'Fwd IAT Std' 'Bwd IAT Mean' 'Bwd IAT Std'\n 'SYN Flag Count' 'ACK Flag Count' 'FIN Flag Count' 'RST Flag Count'\n 'PSH Flag Count' 'URG Flag Count']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"test","metadata":{}}]}